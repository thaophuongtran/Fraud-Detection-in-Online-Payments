# -*- coding: utf-8 -*-
"""Fraud Detection in Online Payments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QeMHHsWcApAi2Id_ov4SdTIaBUsKevq0

# Fraud Detection in Online Payments

### Thao Tran 


---


In this article, we employ different machine learning models to identify fraud from online transactions data. The data comes from Kaggle, ["Online Payments Fraud Detection Dataset"](https://www.kaggle.com/datasets/rupakroy/online-payments-fraud-detection-dataset?resource=download), and has the following columns:

* `step`: represents a unit of time where 1 step equals 1 hour
* `type`: type of online transaction
* `amount`: the amount of the transaction
* `nameOrig`: customer starting the transaction
* `oldbalanceOrg`: balance before the transaction
* `newbalanceOrig`: balance after the transaction
* `nameDest`: recipient of the transaction
* `oldbalanceDest`: initial balance of recipient before the transaction
* `newbalanceDest`: the new balance of recipient after the transaction
* `isFraud`: fraud transaction

We start by exploring and cleaning the dataset.
"""

# load library 
import pandas as pd
import numpy as np
import sklearn
import scipy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
import sklearn.model_selection
# ml models
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
# performance metrics
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
# ann
import keras
from keras.models import Sequential
from keras.wrappers.scikit_learn import KerasClassifier
from keras.layers import Dense
# resampling 
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# load data
pms = pd.read_csv("PS_20174392719_1491204439457_log.csv")
pms.describe()

# define one hot encoding
encoder = OneHotEncoder(sparse=False)
# drop name vars
pms = pms.drop(['nameOrig','nameDest'],axis=1)
# one hot encoding type variable 
onehot = pd.DataFrame(encoder.fit_transform(pms[['type']]))
# rename columns
onehot.columns=['Cashin','Cashout','Debit','Payment','Transfer']
# join new type variables
pms = pms.join(onehot)
pms.head()

"""## Undersampling 

In this classification problem, our target variable is `isFraud`. Unsurprisingly, it is imbalanced where we only have 8,213 cases of fraud out of roughly 6.36 million onservations. 
"""

pms.isFraud.value_counts()

"""Thus we implement  under sampling method to balance out fraud observations however we retain the skewed distribution by keep the majority class to have five times more observations than the minority class. We then split the data into training set (60 percent) and testing set (40 percent) and standardize the data. """

# split data to X and y 
X = pms.select_dtypes(include=np.number).drop(['isFraud'],axis=1)
y = pms.isFraud

# instantiating the random undersampler
rus = RandomUnderSampler(sampling_strategy=0.2) 
# resampling X, y
X_rus, y_rus = rus.fit_resample(X, y)
# check fraud cases count
print(y_rus.value_counts())

# split data into train and test set 
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_rus, y_rus, test_size = 0.4, random_state = 0)
# normalize the data 
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Machine Learning Models Comparisons

Then we apply different classification models:

* Logistics Regression
* Support Vector Machine (SVM)
* Random Forest
* Naive Bayes
* Artificial Neural Networks (ANN)

We especially have high performance expectations for SVM and ANN as they historically performed well with imbalanced data and skewed distributions. The performance metrics to be compared across models in this analysis includes the confusion matrix, accuracy score, and the ROC curve.   
"""

# logistic regression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
# confusion matrix 
print("Confusion matrix: \n",confusion_matrix(y_test, logreg.predict(X_test)))
# accuracy score
print("Accuracy score: ",accuracy_score(y_test, logreg.predict(X_test)))
# roc curve
metrics.plot_roc_curve(logreg, X_test, y_test)

# svm
svms = svm.SVC()
svms.fit(X_train,y_train)
# confusion matrix 
print("Confusion matrix: \n",confusion_matrix(y_test, svms.predict(X_test)))
# accuracy score
print("Accuracy score: ",accuracy_score(y_test, svms.predict(X_test)))
# roc curve
metrics.plot_roc_curve(svms, X_test, y_test)

# random forest 
clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(X_train, y_train)
# confusion matrix 
print("Confusion matrix: \n",confusion_matrix(y_test, clf.predict(X_test)))
# accuracy score
print("Accuracy score: ",accuracy_score(y_test, clf.predict(X_test)))
# roc curve
metrics.plot_roc_curve(clf, X_test, y_test)

# naive bayes
gnb = GaussianNB()
gnb.fit(X_train, y_train)
# confusion matrix 
print("Confusion matrix: \n",confusion_matrix(y_test, gnb.predict(X_test)))
# accuracy score
print("Accuracy score: ",accuracy_score(y_test, gnb.predict(X_test)))
# roc curve
metrics.plot_roc_curve(gnb, X_test, y_test)

#  artificial neural network
classifier = Sequential()
classifier.add(Dense(12, activation = 'softmax',input_dim=12))
classifier.add(Dense(6, activation = 'relu'))
classifier.add(Dense(1, activation = 'sigmoid'))
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)
# confusion matrix 
print("Confusion matrix: \n",confusion_matrix(y_test, (classifier.predict(X_test)>0.5)))
# accuracy score
print("Accuracy score: ",accuracy_score(y_test, (classifier.predict(X_test)>0.5)))

"""All models perform well with accuracy over 90 percent and AUC over 93 percent. According to the performance metrics, particularly the accuracy scores, ANN (accuracy = 0.9755) significantly outperforms other models, followed by random forest (accuracy - 0.9528). We thus proceed to train the model on the entire data set to be used for further classification.   """

#  artificial neural network on the whole data sets
classifier = Sequential()
classifier.add(Dense(12, activation = 'softmax',input_dim=12))
classifier.add(Dense(6, activation = 'relu'))
classifier.add(Dense(1, activation = 'sigmoid'))
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
classifier.fit(X, y, batch_size = 1000, epochs = 10)